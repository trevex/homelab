# FRR expects to have these files owned by frr:frr on startup.
# Having them in a ConfigMap allows us to modify behaviors: for example enabling more daemons on startup.
apiVersion: v1
kind: ConfigMap
metadata:
  name: vtep
  namespace: kube-system
data:
  daemons: |
    # This file tells the frr package which daemons to start.
    #
    # Sample configurations for these daemons can be found in
    # /usr/share/doc/frr/examples/.
    #
    # ATTENTION:
    #
    # When activating a daemon for the first time, a config file, even if it is
    # empty, has to be present *and* be owned by the user and group "frr", else
    # the daemon will not be started by /etc/init.d/frr. The permissions should
    # be u=rw,g=r,o=.
    # When using "vtysh" such a config file is also needed. It should be owned by
    # group "frrvty" and set to ug=rw,o= though. Check /etc/pam.d/frr, too.
    #
    # The watchfrr and zebra daemons are always started.
    #
    bgpd=yes
    ospfd=no
    ospf6d=no
    ripd=no
    ripngd=no
    isisd=no
    pimd=no
    ldpd=no
    nhrpd=no
    eigrpd=no
    babeld=no
    sharpd=no
    pbrd=no
    bfdd=yes
    fabricd=no
    vrrpd=no

    #
    # If this option is set the /etc/init.d/frr script automatically loads
    # the config via "vtysh -b" when the servers are started.
    # Check /etc/pam.d/frr if you intend to use "vtysh"!
    #
    vtysh_enable=yes
    zebra_options="  -A 127.0.0.1 -s 90000000"
    bgpd_options="   -A 127.0.0.1"
    ospfd_options="  -A 127.0.0.1"
    ospf6d_options=" -A ::1"
    ripd_options="   -A 127.0.0.1"
    ripngd_options=" -A ::1"
    isisd_options="  -A 127.0.0.1"
    pimd_options="   -A 127.0.0.1"
    ldpd_options="   -A 127.0.0.1"
    nhrpd_options="  -A 127.0.0.1"
    eigrpd_options=" -A 127.0.0.1"
    babeld_options=" -A 127.0.0.1"
    sharpd_options=" -A 127.0.0.1"
    pbrd_options="   -A 127.0.0.1"
    staticd_options="-A 127.0.0.1"
    bfdd_options="   -A 127.0.0.1"
    fabricd_options="-A 127.0.0.1"
    vrrpd_options="  -A 127.0.0.1"

    # configuration profile
    #
    #frr_profile="traditional"
    #frr_profile="datacenter"

    #
    # This is the maximum number of FD's that will be available.
    # Upon startup this is read by the control files and ulimit
    # is called. Uncomment and use a reasonable value for your
    # setup if you are expecting a large number of peers in
    # say BGP.
    #MAX_FDS=1024

    # The list of daemons to watch is automatically generated by the init script.
    #watchfrr_options=""

    # for debugging purposes, you can specify a "wrap" command to start instead
    # of starting the daemon directly, e.g. to use valgrind on ospfd:
    #   ospfd_wrap="/usr/bin/valgrind"
    # or you can use "all_wrap" for all daemons, e.g. to use perf record:
    #   all_wrap="/usr/bin/perf record --call-graph -"
    # the normal daemon command is added to this at the end.
  vtysh.conf: |+
    service integrated-vtysh-config
  compute-1.frr.conf: |+
    frr version 7.5.1
    frr defaults traditional
    hostname vtep-controller-1
    line vty
    log file /etc/frr/frr.log informational
    !
    ip nht resolve-via-default
    !
    router bgp 65000
      bgp router-id 192.168.1.122
      no bgp default ipv4-unicast
      neighbor fabric peer-group
      neighbor fabric remote-as 65000
      neighbor fabric version 4+
      ! BGP sessions with route reflectors
      neighbor 192.168.1.121 peer-group fabric
      !
      address-family l2vpn evpn
       neighbor fabric activate
       advertise-all-vni
      exit-address-family
      !
    !
  compute-2.frr.conf: |+
    frr version 7.5.1
    frr defaults traditional
    hostname vtep-controller-1
    line vty
    log file /etc/frr/frr.log informational
    !
    ip nht resolve-via-default
    !
    router bgp 65000
      bgp router-id 192.168.1.123
      no bgp default ipv4-unicast
      neighbor fabric peer-group
      neighbor fabric remote-as 65000
      neighbor fabric version 4+
      ! BGP sessions with route reflectors
      neighbor 192.168.1.121 peer-group fabric
      !
      address-family l2vpn evpn
       neighbor fabric activate
       advertise-all-vni
      exit-address-family
      !
    !
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: vtep
  namespace: kube-system
  labels:
    app.kubernetes.io/name: vtep
    app.kubernetes.io/instance: vtep
    app.kubernetes.io/component: vtep
    app.kubernetes.io/created-by: vtep
    app.kubernetes.io/part-of: vtep
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: vtep
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: frr
      labels:
        app.kubernetes.io/name: vtep
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/control-plane
                operator: NotIn
                values:
                - "true"
      containers:
      - name: frr
        securityContext:
          capabilities:
            add: ["NET_ADMIN", "NET_RAW", "SYS_ADMIN", "NET_BIND_SERVICE"]
        image: quay.io/frrouting/frr:8.4.2
        env:
          - name: TINI_SUBREAPER
            value: "true"
        volumeMounts:
          - name: frr-sockets
            mountPath: /var/run/frr
          - name: frr-conf
            mountPath: /etc/frr
        # The command is FRR's default entrypoint & waiting for the log file to appear and tailing it.
        # If the log file isn't created in 60 seconds the tail fails and the container is restarted.
        # This workaround is needed to have the frr logs as part of kubectl logs -c frr < k8s-frr-podname >.
        command:
          - /bin/sh
          - -c
          - |
            /sbin/tini -- /usr/lib/frr/docker-start &
            attempts=0
            until [[ -f /etc/frr/frr.log || $attempts -eq 60 ]]; do
              sleep 1
              attempts=$(( $attempts + 1 ))
            done
            tail -f /etc/frr/frr.log
        livenessProbe:
          httpGet:
            path: /livez
            port: 7573
            host: 127.0.0.1
          periodSeconds: 5
          failureThreshold: 3
        startupProbe:
          httpGet:
            path: /livez
            port: 7573
            host: 127.0.0.1
          failureThreshold: 30
          periodSeconds: 5
      - name: frr-metrics
        image: quay.io/frrouting/frr:8.4.2
        command: ["/etc/frr_metrics/frr-metrics"]
        args:
          - --metrics-port=7573
          - --metrics-bind-address=0.0.0.0
        ports:
          - containerPort: 7573
            name: monitoring
        volumeMounts:
          - name: frr-sockets
            mountPath: /var/run/frr
          - name: frr-conf
            mountPath: /etc/frr
          - name: metrics
            mountPath: /etc/frr_metrics
      initContainers:
        # Copies the initial config files with the right permissions to the shared volume.
        - name: cp-frr-files
          securityContext:
            runAsUser: 100
            runAsGroup: 101
          image: quay.io/frrouting/frr:8.4.2
          command: ["/bin/sh", "-c", "cp -Lf /tmp/frr/daemons /etc/frr/daemons && cp /tmp/frr/vtysh.conf /etc/frr/vtysh.conf && cp /tmp/frr/$NODE_NAME.frr.conf /etc/frr/frr.conf"]
          env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          volumeMounts:
            - name: frr-startup
              mountPath: /tmp/frr
            - name: frr-conf
              mountPath: /etc/frr
        - name: cp-metrics
          image: quay.io/metallb/frr-k8s:v0.0.8
          command: ["/bin/sh", "-c", "cp -f /frr-metrics /etc/frr_metrics/"]
          volumeMounts:
            - name: metrics
              mountPath: /etc/frr_metrics
      volumes:
        - name: frr-sockets
          emptyDir: {}
        - name: frr-startup
          configMap:
            name: vtep
        - name: frr-conf
          emptyDir: {}
        - name: reloader
          emptyDir: {}
        - name: metrics
          emptyDir: {}
      terminationGracePeriodSeconds: 10
      shareProcessNamespace: true
      hostNetwork: true
